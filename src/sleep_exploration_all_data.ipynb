{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.ndimage    \n",
    "import scipy.signal    \n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the data: with all subjects: takes a long time (~20min)\n",
    "if not os.path.exists('../data/training/all_data.csv'):\n",
    "    strings = ['b','c','d','e','f']\n",
    "    data = pd.read_csv('../data/training/a_data.csv')\n",
    "    labels = pd.read_csv('../data/training/a_labels.csv')\n",
    "    for s in strings:\n",
    "        print(s)\n",
    "        temp_data = pd.read_csv('../data/training/'+s+'_data.csv')\n",
    "        data = pd.concat([data,temp_data])\n",
    "        temp_labels = pd.read_csv('../data/training/'+s+'_labels.csv')\n",
    "        labels = pd.concat([labels, temp_labels])\n",
    "    data.to_csv('../data/training/all_data.csv')\n",
    "    labels.to_csv('../data/training/all_labels.csv')\n",
    "else:\n",
    "    data = pd.read_csv('../data/training/all_data.csv')\n",
    "    labels = pd.read_csv('../data/training/all_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group datapoints into bins, corresponding to a seconnd of recording time\n",
    "data['TimestampToSec'] = data['Timestamp'].astype(int)\n",
    "grouped = data.groupby('TimestampToSec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Energy(coeffs, k):\n",
    "    return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "import pywt.data\n",
    "mode = pywt.Modes.smooth\n",
    "\n",
    "def signal_decomp(data):\n",
    "    \"\"\"Decompose and plot a signal S.\n",
    "    S = An + Dn + Dn-1 + ... + D1\n",
    "    \"\"\"\n",
    "    w = pywt.Wavelet('db4')\n",
    "    a = data\n",
    "    ca = []\n",
    "    cd = []\n",
    "    for i in range(5):\n",
    "        (a, d) = pywt.dwt(a, w, mode)\n",
    "        ca.append(a)\n",
    "        cd.append(d)  \n",
    "    return ca, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal    \n",
    "\n",
    "# CONSTRUCT FEATURES\n",
    "\n",
    "# for every label, look up the corresponding data\n",
    "features = []\n",
    "for l in range(len(labels)):\n",
    "    try:\n",
    "        time = labels['Timestamp'][l]\n",
    "        slice = grouped.get_group(time)\n",
    "    except KeyError:\n",
    "        print(time)\n",
    "        pass\n",
    "    # for every channel\n",
    "    power_all_channels = []\n",
    "    # 1-7 EEG, 8th channel is ECG data\n",
    "    for ch in range(8):\n",
    "        single_sec_ch = slice['Ch{}'.format(ch)]\n",
    "        \n",
    "        # median filter the data\n",
    "        pre_processed = scipy.signal.medfilt(single_sec_ch, kernel_size=3)  \n",
    "        \n",
    "        _, cd = signal_decomp(pre_processed)\n",
    "        # for every decomp. level\n",
    "        power = []\n",
    "        for l in range(5):\n",
    "            power.append(Energy(cd, l))\n",
    "            \n",
    "        # collect power for all channels into one vector \n",
    "        power_all_channels.append(power) \n",
    "    # currently mean power of the frequency bands over all channels are the only features\n",
    "    power_vec = np.asarray(power_all_channels).flatten()\n",
    "    features.append(power_vec)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.asarray(features).shape)\n",
    "target_names = np.unique(labels['Event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_train,y_train,X_val,y_val = train_test_split(\n",
    "...     features, label['Event'], test_size=0.33, random_state=0)\n",
    "\n",
    "pickle.dump(X_train,open(\"../data/training/features_train.p\", \"wb\"))\n",
    "pickle.dump(y_train,open(\"../data/training/targets_train.p\", \"wb\"))\n",
    "pickle.dump(X_val,open(\"../data/training/features_validation.p\", \"wb\"))\n",
    "pickle.dump(y_val,open(\"../data/training/targets_validation.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_train =pickle.load(open(\"../data/training/features_train.p\", \"rb\"))\n",
    "y_train =pickle.load(open(\"../data/training/targets_train.p\", \"rb\"))\n",
    "X_val =pickle.load(open(\"../data/training/features_validation.p\", \"rb\"))\n",
    "y_val =pickle.load(open(\"../data/training/targets_validation.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=n_iter_search):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Random Forst\n",
    "clf = ensemble.RandomForestClassifier(n_estimators = 10, criterion='entropy', class_weight='balanced', n_jobs = -1)\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\":sp_randint(1, 100),\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 40),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "report(random_search.cv_results_)\n",
    "#clf = clf.fit(features, labels['Event'])\n",
    "#print(clf.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'n_estimators': 61\n",
    "\n",
    "## Random Forst\n",
    "clf = ensemble.RandomForestClassifier(n_estimators = 73, criterion='entropy', class_weight='balanced', max_features=21, n_jobs = -1)\n",
    "\n",
    "\n",
    "predicted = cross_val_predict(clf, X_train, y_train, cv=10)\n",
    "\n",
    "acc = metrics.accuracy_score(y_train, predicted)\n",
    "print(\"This is the Score: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(predicted,open(\"/home/felix/Desktop/forest_predictions.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=18)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=18)\n",
    "    plt.yticks(tick_marks, classes, fontsize=18)\n",
    "\n",
    "    if normalize:\n",
    "        float_formatter = lambda x: \"%.2f\" % x\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, float_formatter(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "        else:\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names, counts = np.unique(y_train, return_counts=True)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_train, predicted)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "\n",
    "## AdaBoost\n",
    "clf = ensemble.AdaBoostClassifier()\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\":sp_randint(50, 250),\n",
    "              \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "              \"base_estimator\": [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3)]\n",
    "             }\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "report(random_search.cv_results_)\n",
    "#clf = clf.fit(features, labels['Event'])\n",
    "#print(clf.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'n_estimators': 61\n",
    "\n",
    "## Random Forst\n",
    "clf = ensemble.AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3),n_estimators=188, algorithm ='SAMME.R')\n",
    "\n",
    "\n",
    "predicted = cross_val_predict(clf, X_train, y_train, cv=10)\n",
    "\n",
    "acc = metrics.accuracy_score(y_train, predicted)\n",
    "print(\"This is the Score: {}\".format(acc))\n",
    "\n",
    "class_names, counts = np.unique(y_train, return_counts=True)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_train, predicted)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_clf = ensemble.RandomForestClassifier(n_estimators = 73, criterion='entropy', class_weight='balanced', max_features=21, n_jobs = -1)\n",
    "forest_clf.fit(X_train,y_train)\n",
    "forest_pred = forest_clf.predict(X_val)\n",
    "\n",
    "acc = metrics.accuracy_score(y_val, forest_pred)\n",
    "print(\"This is the Score: {}\".format(acc))\n",
    "\n",
    "class_names, counts = np.unique(y_val, return_counts=True)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_val, forest_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix without normalization for validation set')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix for validation set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_clf = ensemble.AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3), n_estimators=188, algorithm ='SAMME.R')\n",
    "ada_clf.fit(X_train,y_train)\n",
    "ada_pred = ada_clf.predict(X_val)\n",
    "\n",
    "acc = metrics.accuracy_score(y_val, ada_pred)\n",
    "print(\"This is the Score: {}\".format(acc))\n",
    "\n",
    "class_names, counts = np.unique(y_val, return_counts=True)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_val, ada_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix without normalization for validation set')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix for validation set')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
