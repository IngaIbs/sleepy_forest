{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for the sleep data project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "The purpose of this project is to use ensemble methods to discriminate between different sleep stages from EEG sleep data. We experiment with different ensemble methods and parameters in order to compare the results and find the most appropriate approach given our data.\n",
    "\n",
    "We first prepare and filter the data for further use. Then perform Wavelet decomposition to identify and extract different frequencies from the data. Subsequently, we do feature extraction, where features correspond to the power of the frequency bands. Afterwards, we train and test two different ensemble methods; Random Forest and Adaboost. In order to optimize these results we then perform hyperparameter search and compare the results. To finish we propose and implement further methods to optimize classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep Data Description\n",
    "\n",
    "Data was collected with the Traumschreiber, high-tech sleep mask developed for research purposes.\n",
    "\n",
    "The data used to train and test the classifier consists of five data sets corresponding to different nights of sleep. Each data set containing information from seven Electroencephalogram (EEG) channels and one Electrocardiogram (ECG) channel, recorded for about seven hours of sleep.\n",
    "\n",
    "Data is labeled by epochs of one second, where each second contains about 200 microvolt points. These labels correspond to the sleep stages introduced by the American Academy of Sleep Medicine (AASM) that differentiates between five main sleeping stages: \n",
    "\n",
    "(1) Wakefulness: Active wakefulness with beta waves (+13 Hz) and relaxed wakefulness with mostly alpha wave (8-13 Hz).\n",
    "(2) Non-Rapid Eye Movement (NREM) 1: Dominated by Theta activity (4-7 Hz).\n",
    "(3) NREM-2: Characterized by Theta waves, sleep spindles and K-complexes.\n",
    "(4) NREM-3: Dominated by Delta wave (0.5-2 Hz) along with some sleep spindles.\n",
    "(5) Rapid Eye Movement (REM): Characterized by low-amplitude mixed-frequency brain waves. Theta, alpha and even beta activity can be observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.ndimage    \n",
    "import scipy.signal    \n",
    "import os\n",
    "from feature_extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data for subject a\n",
    "data = pd.read_csv('../data/by_subject/a_data.csv')\n",
    "labels = pd.read_csv('../data/by_subject/a_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median filter justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data presented huge peaks that where probably product of interference in the Bluetooth signal. To eliminate peaks while altering the data as little as possible, we decided to implement a median filter by using the Scikit median filter function, with a Kernel size of three. This filter runs through the signal entry by entry, replacing each entry with the median of neighboring entries. The pattern of neighbors is referred as the Kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example Plot from data set 1, EEG channel 0\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(data['Ch0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(data['Ch0'][1034300:1034500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Median filter implementation loop\n",
    "pre_processed = scipy.signal.medfilt(data['Ch0'], kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(pre_processed[1034300:1034500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group datapoints into bins, corresponding to a second of recording time maybe mit preprocessing\n",
    "data['TimestampToSec'] = data['Timestamp'].astype(int)\n",
    "grouped = data.groupby('TimestampToSec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a second of data of all channels\n",
    "\n",
    "single_sec_data = grouped.get_group(1489016350)\n",
    "single_sec_ch = single_sec_data['Ch0']\n",
    "\n",
    "#plt.plot(single_sec_ch)\n",
    "plt.plot(single_sec_data['Ch0'])\n",
    "plt.plot(single_sec_data['Ch1'])\n",
    "plt.plot(single_sec_data['Ch2'])\n",
    "plt.plot(single_sec_data['Ch3'])\n",
    "plt.plot(single_sec_data['Ch4'])\n",
    "plt.plot(single_sec_data['Ch5'])\n",
    "plt.plot(single_sec_data['Ch6'])\n",
    "plt.plot(single_sec_data['Ch7'])\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Wavelet Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Wavelet Transform Overview\n",
    "The wavelet are waves of irregular form in shape and compactly supported. These properties along with the main two operations of scaling and shifting, which produce a time-scale representation of the signal, make wavelets an ideal tool for analysing signals of non-stationary nature. Their irregular shape makes them suitable for analysing signals with discontinuities, and their compactly supported nature enables temporal localisation. Motivated by the adaptive time-frequency resolution properties of the Wavelet Transform and the corresponding fact that some stages in sleep recordings have a well defined time-frequency domain we opted to use Discrete Wavelet Decomposition to obtain five sub-bands of the original signal and consequently performed feature extraction on them for the classification.\n",
    "\n",
    "The Discrete Wavelet Decomposition algorithm we implemented relays firstly on a dyadic scaling of the wavelength and secondly on a discrete shifting across the original signal. The first operation serves as half band filter which halves the highest frequency component of the original signal,providing  lower computational time and less memory usage. This in accordance to Nyquistâ€™s sampling rate allowing the usage of half of the previous sample points at each level of the decomposition for a proper reconstruction of the original signal. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wavelet transform EEG ERD ERS event-related potentials time frequencya.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for sleep classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = pywt.Modes.smooth\n",
    "\n",
    "def signal_decomp(data):\n",
    "    \"\"\"Decompose and plot a signal S.\n",
    "    S = An + Dn + Dn-1 + ... + D1\n",
    "    \"\"\"\n",
    "    w = pywt.Wavelet('db4')\n",
    "    a = data\n",
    "    ca = []\n",
    "    cd = []\n",
    "    for i in range(5):\n",
    "        (a, d) = pywt.dwt(a, w, mode)\n",
    "        ca.append(a)\n",
    "        cd.append(d)  \n",
    "    return ca, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Energy(coeffs, k):\n",
    "    return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Decomposition\n",
    "The algorithm adopted can be better visualized as a tree of low and high pass filter, which perform the decomposition of the signal into different frequency bands applying successive filtering of the time domain signal. \n",
    "The original signal is successively decomposed into components of lower resolution, while the high frequency components are not analysed any further.This decomposition halves the time resolution since only half the number of samples now characterizes the entire signal. \n",
    "However it doubles the frequency resolution, since the frequency band of the signal now spans only half the previous.The maximum depth of decomposition is dependent on the input size of the data to be analysed, with 2N data samples enabling the breakdown of the signal into N discrete levels using the discrete wavelet transform. This procedure thus offers a good time resolution at high frequencies, and good frequency resolution at low frequencies. \n",
    "\n",
    "This matches well the resolution of each sub-band with certain sleep stages patterns, for example capturing at an higher time resolution the signal of the beta stage, which shows abrupt discontinuities .\n",
    "We discarded the first two levels of the decomposition simply because those frequency bands are completely absent it the original signal. In conclusion the frequencies that are most prominent in the original signal will appear as high amplitudes in the corresponding region of the Discrete Wavelet Transform signal that includes those particular frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Untitled Diagram (3).png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_signal_decomp(data, w, title):\n",
    "    ca, cd = signal_decomp(data)\n",
    "        \n",
    "    rec_a = []\n",
    "    rec_d = []\n",
    "\n",
    "    for i, coeff in enumerate(ca):\n",
    "        coeff_list = [coeff, None] + [None] * i\n",
    "        rec_a.append(pywt.waverec(coeff_list, w))\n",
    "\n",
    "    for i, coeff in enumerate(cd):\n",
    "        coeff_list = [None, coeff] + [None] * i\n",
    "        rec_d.append(pywt.waverec(coeff_list, w))\n",
    "\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    ax_main = fig.add_subplot(len(rec_a) + 1, 1, 1)\n",
    "    ax_main.set_title(title, fontsize=20)\n",
    "    ax_main.plot(data)\n",
    "    ax_main.set_xlim(data.index[0], data.index[len(data) - 1])\n",
    "\n",
    "    for i, y in enumerate(rec_a):\n",
    "        ax = fig.add_subplot(len(rec_a) + 1, 2, 3 + i * 2)\n",
    "        ax.plot(y, 'r')\n",
    "        ax.set_xlim(0, len(y) - 1)\n",
    "        ax.set_ylabel(\"A%d\" % (i + 1))\n",
    "\n",
    "    for i, y in enumerate(rec_d):\n",
    "        ax = fig.add_subplot(len(rec_d) + 1, 2, 4 + i * 2)\n",
    "        ax.plot(y, 'g')\n",
    "        ax.set_xlim(0, len(y) - 1)\n",
    "        ax.set_ylabel(\"D%d\" % (i + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_signal_decomp(single_sec_ch, 'db4', \"Single Sec single Channel EEG data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignore from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONSTRUCT FEATURES\n",
    "\n",
    "# for every label, look up the corresponding data\n",
    "features = []\n",
    "for l in range(len(labels)):\n",
    "    try:\n",
    "        time = labels['Timestamp'][l]\n",
    "        slice = grouped.get_group(time)\n",
    "    except KeyError:\n",
    "        print(time)\n",
    "        pass\n",
    "    # for every channel\n",
    "    power_all_channels = []\n",
    "    # 1-7 EEG, 8th channel is ECG data\n",
    "    for ch in range(8):\n",
    "        single_sec_ch = slice['Ch{}'.format(ch)]\n",
    "        \n",
    "        # median filter the data\n",
    "        pre_processed = scipy.signal.medfilt(single_sec_ch, kernel_size=3)  \n",
    "        \n",
    "        _, cd = signal_decomp(pre_processed)\n",
    "        # for every decomp. level\n",
    "        power = []\n",
    "        for l in range(5):\n",
    "            power.append(Energy(cd, l))\n",
    "            \n",
    "        # collect power for all channels into one vector \n",
    "        power_all_channels.append(power) \n",
    "    # currently mean power of the frequency bands over all channels are the only features\n",
    "    power_vec = np.asarray(power_all_channels).flatten()\n",
    "    features.append(power_vec)\n",
    "features =np.asarray(features)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the features\n",
    "If no features are available run the feature_extractor.py to get the feature files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.cross_validation import LabelKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature statistics\n",
    "\n",
    "In order to gain an insight into the variance we have over subjects we take a look at the mean and standard deviation of each subject. Below the mean and standard deviation for all of the 40 features are plotted, for the whole data and for each sleep phase. Each line corresponds to one subject.  We can see that the first and the last five features, corresponding to the first and last channel show the biggest differences in means and standard deviation, whereas the other channels are relatively similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the feature and target table\n",
    "features = pd.read_csv(\"../data/precomputed_features/features.csv\")\n",
    "targets = pd.read_csv(\"../data/precomputed_features/targets.csv\")\n",
    "\n",
    "# merge the tables in order to derive the sleep phase later\n",
    "merge = pd.merge(features,targets)\n",
    "\n",
    "# group the data by each subject\n",
    "grouped = merge.groupby('40')\n",
    "\n",
    "# get the sleep phase identifier\n",
    "events = merge['Event'].unique()\n",
    "\n",
    "means = []\n",
    "means_phase = np.zeros(shape=(6,5, 40))\n",
    "stds_phase =  np.zeros(shape=(6,5,40))\n",
    "stds = []\n",
    "\n",
    "for i in range(6):\n",
    "    g = grouped.get_group(i)\n",
    "    \n",
    "    del g['Timestamp']\n",
    "    del g['40']\n",
    "    del g['subject_id']\n",
    "    for j in range(events.shape[0]):\n",
    "        p = g[g['Event']==events[j]]\n",
    "        del p['Event']\n",
    "        means_phase[i,j, :] = p.describe().ix['mean']\n",
    "        stds_phase[i,j, :] = p.describe().ix['std']\n",
    "        \n",
    "    del g['Event']\n",
    "    means.append(g.describe().ix['mean'])\n",
    "    stds.append(g.describe().ix['std'])\n",
    "        \n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.set_title('Means of the features for each subject', fontsize = 25)\n",
    "ax1.plot(np.transpose(np.asarray(means)))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2. set_title('Stds of the features for each subject', fontsize = 25)\n",
    "ax2.plot(np.transpose(np.asarray(stds)))\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(20,20),facecolor='w', edgecolor='k')\n",
    "fig.suptitle('Means for each sleep phase', fontsize = 25)\n",
    "for i in range(5):\n",
    "    temp = 320+i+1\n",
    "    ax=plt.subplot(temp)\n",
    "    ax.set_title(events[i], fontsize = 25)\n",
    "    ax.plot(np.transpose(means_phase[:,i,:]))\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(20,20),facecolor='w', edgecolor='k')\n",
    "fig.suptitle('Stds for each sleep phase', fontsize = 25)\n",
    "\n",
    "for i in range(5):\n",
    "    temp = 320+i+1\n",
    "    ax=plt.subplot(temp)\n",
    "    ax.set_title(events[i], fontsize = 25)\n",
    "    ax.plot(np.transpose(stds_phase[:,i,:]))\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throw out first and last channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"../data/precomputed_features/features.csv\")\n",
    "targets = pd.read_csv(\"../data/precomputed_features/targets.csv\")\n",
    "\n",
    "\n",
    "features = features.drop(features.columns[[1,2,3,4,5,36,37,38,39,40]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a separate test set to test our classifiers on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Test set subject a\n",
    "X_test = features[features['40']==0]\n",
    "y_test = targets[targets['subject_id']==0]\n",
    "\n",
    "# Training set\n",
    "X_train = features[features['40'] > 0]\n",
    "y_train =  targets[targets['subject_id'] > 0]\n",
    "\n",
    "#X_train,X_test,y_train,y_test = train_test_split(features, targets['stages'], test_size=0.99, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Set up\n",
    "The general set up consists of a 4 fold crossvalidation (splitting always one subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores for Random Search\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# only 5 subjects in the dataset\n",
    "cv_labels = y_train['subject_id']\n",
    "lkf = LabelKFold(cv_labels, n_folds=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "\n",
    "## Random Forst\n",
    "clf = ensemble.RandomForestClassifier(n_estimators = 10, criterion='entropy', class_weight='balanced', n_jobs = -1)\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\":sp_randint(1, 100),\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 40),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv = lkf)\n",
    "\n",
    "random_search.fit(X_train, y_train['Event'])\n",
    "\n",
    "report(random_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'n_estimators': 61\n",
    "\n",
    "## Random Forst\n",
    "rf_clf = ensemble.RandomForestClassifier(n_estimators = 73, criterion='entropy', class_weight='balanced', max_features=1, n_jobs = -1)\n",
    "\n",
    "\n",
    "rf_predicted = cross_val_predict(rf_clf, X_train, y_train['Event'], cv=lkf)\n",
    "\n",
    "rf_acc = metrics.accuracy_score(y_train['Event'], rf_predicted)\n",
    "print(\"This is the Score: {}\".format(rf_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "\n",
    "## AdaBoost\n",
    "clf = ensemble.AdaBoostClassifier()\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\":sp_randint(50, 250),\n",
    "              \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "              \"base_estimator\": [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3)]\n",
    "             }\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,cv = lkf)\n",
    "\n",
    "random_search.fit(X_train, y_train['Event'])\n",
    "\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'n_estimators': 61\n",
    "\n",
    "## Random Forst\n",
    "ada_clf = ensemble.AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3),n_estimators=188, algorithm ='SAMME.R')\n",
    "\n",
    "\n",
    "ada_predicted = cross_val_predict(ada_clf, X_train, y_train['Event'], cv=lkf)\n",
    "\n",
    "ada_acc = metrics.accuracy_score(y_train['Event'], ada_predicted)\n",
    "print(\"This is the Score: {}\".format(ada_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize false predictions\n",
    "def vis_clfs(targets, predicted):\n",
    "    # color coding grayscale\n",
    "    #color = {'stage_q_N34' : 0, 'stage_q_N23': 50, 'stage_q_N12': 100, 'stage_q_REM1': 150, 'stage_q_Wake0': 200}\n",
    "    \n",
    "    color = [0,50,100,150,200]\n",
    "    label_text = ['N34', 'N23', 'N12', 'REM', 'Wake']\n",
    "    false_pred = np.where(predicted != targets)\n",
    "    print(false_pred[0].shape)\n",
    "    timepoints = range(0,len(predicted))\n",
    "\n",
    "    rows = np.ceil((len(predicted) / 500)).astype(int) * 10\n",
    "    cols = 500\n",
    "    image = np.ones((rows,cols), dtype=np.int16) * 255\n",
    "\n",
    "    for timepoint in timepoints:\n",
    "        x = timepoint % 500 \n",
    "        y = int(timepoint / 500) * 10\n",
    "        if(np.any(false_pred[0]==timepoint)):\n",
    "            image[y:y+10,x] = 255\n",
    "        else:\n",
    "            image[y:y+10,x] = color[predicted[timepoint]]\n",
    "\n",
    "    \n",
    "    import matplotlib.patches as mpatches\n",
    "    plt.figure(frameon=False, figsize=(16,16))  \n",
    "    plt.title('Classification Results', fontsize=18)\n",
    "    plt.axis('off')   \n",
    "    im = plt.imshow(image,cmap=plt.cm.bone, vmin = 0, vmax = 255)\n",
    "    # get the colors of the values, according to the \n",
    "    # colormap used by imshow\n",
    "    values = [0,50,100,150,200]\n",
    "    colors = [im.cmap(im.norm(value)) for value in values]\n",
    "    # create a patch (proxy artist) for every color \n",
    "    patches = [ mpatches.Patch(color=colors[i], label=\"{l}\".format(l=label_text[i]) ) for i in range(len(values)) ]\n",
    "    # put those patched as legend-handles into the legend\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=18 )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def smooth(probabilities) :\n",
    "    res = np.zeros(probabilities.shape)\n",
    "    # sum over the last n timepoints\n",
    "    res[0,:] = np.squeeze(0.2 * np.ones((5,1)))\n",
    "    for timep in range(probabilities.shape[0]):\n",
    "        # from the second timepoint onwards\n",
    "        if(timep > 0):\n",
    "            prod = res[timep-1,:] * probabilities[timep,:]\n",
    "            norm = np.sum(prod)\n",
    "            res[timep,:] = ((prod / norm + 0.00002)*0.2 + probabilities[timep,:]*0.8)\n",
    "            \n",
    "    # assign class according to the most ofen occuring class within last 5 predictions\n",
    "    classes = np.argmax(res,axis=1)\n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train,y_train['Event'])\n",
    "\n",
    "probabilities = rf_clf.predict_proba(X_test)\n",
    "new = smooth(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_targets = y_test['Event'].apply(lambda v: str(v).replace('stage_q_N12','0').replace('stage_q_N23','1').replace('stage_q_N34','2').replace('stage_q_REM1','3').replace('stage_q_Wake0','4'))\n",
    "#new[0:5\n",
    "number_targets= number_targets.apply(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "acc = metrics.accuracy_score(number_targets, new)\n",
    "print(acc)\n",
    "\n",
    "acc2 = metrics.accuracy_score(y_test['Event'], rf_clf.predict(X_test))\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_clfs(number_targets,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=18)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=18)\n",
    "    plt.yticks(tick_marks, classes, fontsize=18)\n",
    "\n",
    "    if normalize:\n",
    "        float_formatter = lambda x: \"%.2f\" % x\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, float_formatter(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "        else:\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get class names for labels of plot\n",
    "class_names, counts = np.unique(y_train['Event'], return_counts=True)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "rf_cnf_matrix = confusion_matrix(y_train['Event'], rf_predicted)\n",
    "ada_cnf_matrix = confusion_matrix(y_train['Event'], ada_predicted)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_confusion_matrix(rf_cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix for the Random Forest ')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(122)\n",
    "plot_confusion_matrix(ada_cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix for the Ada Boost Classifiers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train,y_train['Event'])\n",
    "ada_clf.fit(X_train,y_train['Event'])\n",
    "\n",
    "\n",
    "rf_pred_test = rf_clf.predict(X_test)\n",
    "ada_pred_test = ada_clf.predict(X_test)\n",
    "\n",
    "\n",
    "rf_acc_test = metrics.accuracy_score(y_test['Event'], rf_pred_test)\n",
    "print(\"This is the Score for Random Forest on the test set: {}\".format(rf_acc_test))\n",
    "\n",
    "ada_acc_test = metrics.accuracy_score(y_test['Event'], ada_pred_test)\n",
    "print(\"This is the Score for Ada-Boost on the test set: {} \\n \\n\".format(ada_acc_test))\n",
    "\n",
    "rf_cnf_matrix_test = confusion_matrix(y_test['Event'], rf_pred_test)\n",
    "ada_cnf_matrix_test = confusion_matrix(y_test['Event'], ada_pred_test)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121)\n",
    "plot_confusion_matrix(rf_cnf_matrix_test, classes=class_names,\n",
    "                      title='Confusion matrix for the Random Forest for the test set')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(122)\n",
    "plot_confusion_matrix(ada_cnf_matrix_test, classes=class_names, normalize=False,\n",
    "                      title='Confusion matrix for the Random Forest for test set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€¢ Wavelet transform allows to adapt and extract relevant information from both time and frequency domain. Because some sleep stages are mainly characterized by the time domain while others by the frequency domain, wavelet decomposition is most appropriate for extracting frequency bands from EEG sleep data.   â€¢ Random Forest and Adaboost preform both over xxx â€¢ Random Forest outperforms Adaboost algorithm by xxx â€¢ Hyperparameter search optimizes the results from both methods significantly. â€¢ Smoothing classification results within sleep stages can help to improve overall results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
