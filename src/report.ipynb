{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for the sleep data project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "The purpose of this project is to use ensemble methods to discriminate between different sleep stages from EEG sleep data. We experiment with different ensemble methods and parameters in order to compare the results and find the most appropriate approach given our data.\n",
    "\n",
    "We first prepare and filter the data for further use. Then perform Wavelet decomposition to identify and extract different frequencies from the data. Subsequently, we do feature extraction, where features correspond to the power of the frequency bands. Afterwards, we train and test two different ensemble methods; Random Forest and Adaboost. In order to optimize these results we then perform hyperparameter search and compare the results. To finish we propose and implement further methods to optimize classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep Data Description\n",
    "\n",
    "Data was collected with the Traumschreiber, high-tech sleep mask developed for research purposes.\n",
    "\n",
    "The data used to train and test the classifier consists of five data sets corresponding to different nights of sleep. Each data set containing information from seven Electroencephalogram (EEG) channels and one Electrocardiogram (ECG) channel, recorded for about seven hours of sleep.\n",
    "\n",
    "Data is labeled by epochs of one second, where each second contains about 200 microvolt points. These labels correspond to the sleep stages introduced by the American Academy of Sleep Medicine (AASM) that differentiates between five main sleeping stages: \n",
    "\n",
    "(1) Wakefulness: Active wakefulness with beta waves (+13 Hz) and relaxed wakefulness with mostly alpha wave (8-13 Hz).\n",
    "(2) Non-Rapid Eye Movement (NREM) 1: Dominated by Theta activity (4-7 Hz).\n",
    "(3) NREM-2: Characterized by Theta waves, sleep spindles and K-complexes.\n",
    "(4) NREM-3: Dominated by Delta wave (0.5-2 Hz) along with some sleep spindles.\n",
    "(5) Rapid Eye Movement (REM): Characterized by low-amplitude mixed-frequency brain waves. Theta, alpha and even beta activity can be observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.ndimage    \n",
    "import scipy.signal    \n",
    "import os\n",
    "import pickle\n",
    "from feature_extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data for subject a\n",
    "data = pd.read_csv('../data/by_subject/a_data.csv')\n",
    "labels = pd.read_csv('../data/by_subject/a_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median filter justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data presented huge peaks that where probably product of interference in the Bluetooth signal. To eliminate peaks while altering the data as little as possible, we decided to implement a median filter by using the Scikit median filter function, with a Kernel size of three. This filter runs through the signal entry by entry, replacing each entry with the median of neighboring entries. The pattern of neighbors is referred as the Kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example Plot from data set 1, EEG channel 0\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(data['Ch0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(data['Ch0'][1034300:1034500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Median filter implementation loop\n",
    "pre_processed = scipy.signal.medfilt(data['Ch0'], kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(pre_processed[1034300:1034500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group datapoints into bins, corresponding to a second of recording time maybe mit preprocessing\n",
    "data['TimestampToSec'] = data['Timestamp'].astype(int)\n",
    "grouped = data.groupby('TimestampToSec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot a second of data of all channels\n",
    "\n",
    "single_sec_data = grouped.get_group(1489016350)\n",
    "single_sec_ch = single_sec_data['Ch0']\n",
    "\n",
    "#plt.plot(single_sec_ch)\n",
    "plt.plot(single_sec_data['Ch0'])\n",
    "plt.plot(single_sec_data['Ch1'])\n",
    "plt.plot(single_sec_data['Ch2'])\n",
    "plt.plot(single_sec_data['Ch3'])\n",
    "plt.plot(single_sec_data['Ch4'])\n",
    "plt.plot(single_sec_data['Ch5'])\n",
    "plt.plot(single_sec_data['Ch6'])\n",
    "plt.plot(single_sec_data['Ch7'])\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Wavelet Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Wavelet Transform Overview\n",
    "The wavelet are waves of irregular form in shape and compactly supported. These properties along with the main two operations of scaling and shifting, which produce a time-scale representation of the signal, make wavelets an ideal tool for analysing signals of non-stationary nature. Their irregular shape makes them suitable for analysing signals with discontinuities, and their compactly supported nature enables temporal localisation. Motivated by the adaptive time-frequency resolution properties of the Wavelet Transform and the corresponding fact that some stages in sleep recordings have a well defined time-frequency domain we opted to use Discrete Wavelet Decomposition to obtain five sub-bands of the original signal and consequently performed feature extraction on them for the classification.\n",
    "\n",
    "The Discrete Wavelet Decomposition algorithm we implemented relays firstly on a dyadic scaling of the wavelength and secondly on a discrete shifting across the original signal. The first operation serves as half band filter which halves the highest frequency component of the original signal,providing  lower computational time and less memory usage. This in accordance to Nyquistâ€™s sampling rate allowing the usage of half of the previous sample points at each level of the decomposition for a proper reconstruction of the original signal. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wavelet transform EEG ERD ERS event-related potentials time frequencya.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for sleep classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = pywt.Modes.smooth\n",
    "\n",
    "def signal_decomp(data):\n",
    "    \"\"\"Decompose and plot a signal S.\n",
    "    S = An + Dn + Dn-1 + ... + D1\n",
    "    \"\"\"\n",
    "    w = pywt.Wavelet('db4')\n",
    "    a = data\n",
    "    ca = []\n",
    "    cd = []\n",
    "    for i in range(5):\n",
    "        (a, d) = pywt.dwt(a, w, mode)\n",
    "        ca.append(a)\n",
    "        cd.append(d)  \n",
    "    return ca, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Energy(coeffs, k):\n",
    "    return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Decomposition\n",
    "The algorithm adopted can be better visualized as a tree of low and high pass filter, which perform the decomposition of the signal into different frequency bands applying successive filtering of the time domain signal. \n",
    "The original signal is successively decomposed into components of lower resolution, while the high frequency components are not analysed any further.This decomposition halves the time resolution since only half the number of samples now characterizes the entire signal. \n",
    "However it doubles the frequency resolution, since the frequency band of the signal now spans only half the previous.The maximum depth of decomposition is dependent on the input size of the data to be analysed, with 2N data samples enabling the breakdown of the signal into N discrete levels using the discrete wavelet transform. This procedure thus offers a good time resolution at high frequencies, and good frequency resolution at low frequencies. \n",
    "\n",
    "This matches well the resolution of each sub-band with certain sleep stages patterns, for example capturing at an higher time resolution the signal of the beta stage, which shows abrupt discontinuities .\n",
    "We discarded the first two levels of the decomposition simply because those frequency bands are completely absent it the original signal. In conclusion the frequencies that are most prominent in the original signal will appear as high amplitudes in the corresponding region of the Discrete Wavelet Transform signal that includes those particular frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Untitled Diagram (3).png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_signal_decomp(data, w, title):\n",
    "    ca, cd = signal_decomp(data)\n",
    "        \n",
    "    rec_a = []\n",
    "    rec_d = []\n",
    "\n",
    "    for i, coeff in enumerate(ca):\n",
    "        coeff_list = [coeff, None] + [None] * i\n",
    "        rec_a.append(pywt.waverec(coeff_list, w))\n",
    "\n",
    "    for i, coeff in enumerate(cd):\n",
    "        coeff_list = [None, coeff] + [None] * i\n",
    "        rec_d.append(pywt.waverec(coeff_list, w))\n",
    "\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    ax_main = fig.add_subplot(len(rec_a) + 1, 1, 1)\n",
    "    ax_main.set_title(title, fontsize=20)\n",
    "    ax_main.plot(data)\n",
    "    ax_main.set_xlim(data.index[0], data.index[len(data) - 1])\n",
    "\n",
    "    for i, y in enumerate(rec_a):\n",
    "        ax = fig.add_subplot(len(rec_a) + 1, 2, 3 + i * 2)\n",
    "        ax.plot(y, 'r')\n",
    "        ax.set_xlim(0, len(y) - 1)\n",
    "        ax.set_ylabel(\"A%d\" % (i + 1))\n",
    "\n",
    "    for i, y in enumerate(rec_d):\n",
    "        ax = fig.add_subplot(len(rec_d) + 1, 2, 4 + i * 2)\n",
    "        ax.plot(y, 'g')\n",
    "        ax.set_xlim(0, len(y) - 1)\n",
    "        ax.set_ylabel(\"D%d\" % (i + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_signal_decomp(single_sec_ch, 'db4', \"Single Sec single Channel EEG data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignore from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONSTRUCT FEATURES\n",
    "\n",
    "# for every label, look up the corresponding data\n",
    "features = []\n",
    "for l in range(len(labels)):\n",
    "    try:\n",
    "        time = labels['Timestamp'][l]\n",
    "        slice = grouped.get_group(time)\n",
    "    except KeyError:\n",
    "        print(time)\n",
    "        pass\n",
    "    # for every channel\n",
    "    power_all_channels = []\n",
    "    # 1-7 EEG, 8th channel is ECG data\n",
    "    for ch in range(8):\n",
    "        single_sec_ch = slice['Ch{}'.format(ch)]\n",
    "        \n",
    "        # median filter the data\n",
    "        pre_processed = scipy.signal.medfilt(single_sec_ch, kernel_size=3)  \n",
    "        \n",
    "        _, cd = signal_decomp(pre_processed)\n",
    "        # for every decomp. level\n",
    "        power = []\n",
    "        for l in range(5):\n",
    "            power.append(Energy(cd, l))\n",
    "            \n",
    "        # collect power for all channels into one vector \n",
    "        power_all_channels.append(power) \n",
    "    # currently mean power of the frequency bands over all channels are the only features\n",
    "    power_vec = np.asarray(power_all_channels).flatten()\n",
    "    features.append(power_vec)\n",
    "features =np.asarray(features)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the features\n",
    "If no features are available run the feature_extractor.py to get the feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"../data/precomputed_features/features.csv\")\n",
    "targets = pd.read_csv(\"../data/precomputed_features/targets.csv\")\n",
    "targets.columns = ['stages']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a separate test set to test our classifiers on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "...     features, targets['stages'], test_size=0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Set up\n",
    " 10 fol crossvalidation, Random Search for hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores for Random Search\n",
    "def report(results, n_top=n_iter_search):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "\n",
    "\n",
    "## Random Forst\n",
    "clf = ensemble.RandomForestClassifier(n_estimators = 10, criterion='entropy', class_weight='balanced', n_jobs = -1)\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\":sp_randint(1, 100),\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 40),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "report(random_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'n_estimators': 61\n",
    "\n",
    "## Random Forst\n",
    "rf_clf = ensemble.RandomForestClassifier(n_estimators = 73, criterion='entropy', class_weight='balanced', max_features=21, n_jobs = -1)\n",
    "\n",
    "\n",
    "rf_predicted = cross_val_predict(rf_clf, X_train, y_train, cv=10)\n",
    "\n",
    "rf_acc = metrics.accuracy_score(y_train, rf_predicted)\n",
    "print(\"This is the Score: {}\".format(rf_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "\n",
    "## AdaBoost\n",
    "clf = ensemble.AdaBoostClassifier()\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\":sp_randint(50, 250),\n",
    "              \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "              \"base_estimator\": [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3)]\n",
    "             }\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'n_estimators': 61\n",
    "\n",
    "## Random Forst\n",
    "ada_clf = ensemble.AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3),n_estimators=188, algorithm ='SAMME.R')\n",
    "\n",
    "\n",
    "ada_predicted = cross_val_predict(ada_clf, X_train, y_train, cv=10)\n",
    "\n",
    "ada_acc = metrics.accuracy_score(y_train, ada_predicted)\n",
    "print(\"This is the Score: {}\".format(ada_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=18)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=18)\n",
    "    plt.yticks(tick_marks, classes, fontsize=18)\n",
    "\n",
    "    if normalize:\n",
    "        float_formatter = lambda x: \"%.2f\" % x\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, float_formatter(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "        else:\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get class names for labels of plot\n",
    "class_names, counts = np.unique(y_train, return_counts=True)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "rf_cnf_matrix = confusion_matrix(y_train, rf_predicted)\n",
    "ada_cnf_matrix = confusion_matrix(y_train, ada_predicted)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_confusion_matrix(rf_cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix for the Random Forest ')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(122)\n",
    "plot_confusion_matrix(ada_cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix for the Ada Boost Classifiers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train,y_train)\n",
    "ada_clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "rf_pred_test = rf_clf.predict(X_test)\n",
    "ada_pred_test = ada_clf.predict(X_test)\n",
    "\n",
    "\n",
    "rf_acc_test = metrics.accuracy_score(y_test, rf_pred_test)\n",
    "print(\"This is the Score for Random Forest on the test set: {}\".format(rf_acc_test))\n",
    "\n",
    "ada_acc_test = metrics.accuracy_score(y_test, ada_pred_test)\n",
    "print(\"This is the Score for Ada-Boost on the test set: {} \\n \\n\".format(ada_acc_test))\n",
    "\n",
    "rf_cnf_matrix_test = confusion_matrix(y_test, rf_pred_test)\n",
    "ada_cnf_matrix_test = confusion_matrix(y_test, ada_pred_test)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121)\n",
    "plot_confusion_matrix(rf_cnf_matrix_test, classes=class_names,\n",
    "                      title='Confusion matrix for the Random Forest for the test set')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(122)\n",
    "plot_confusion_matrix(ada_cnf_matrix_test, classes=class_names, normalize=False,\n",
    "                      title='Confusion matrix for the Random Forest for test set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
